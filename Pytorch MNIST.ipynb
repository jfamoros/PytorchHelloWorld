{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Quadro P1000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import sys\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, base_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        self.base_dir = base_dir\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = idx % 10\n",
    "        index = idx // 10\n",
    "        img_name = str(index) + '.png'\n",
    "        \n",
    "        image = self.load_image(os.path.join('Dataset', 'training', str(label), img_name))\n",
    "        output = [0] * 10\n",
    "        output[label] = 1\n",
    "        \n",
    "        data = {\n",
    "                    'image': torch.FloatTensor(image),\n",
    "                    'label' : torch.LongTensor(output)\n",
    "               }\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def load_image(self, path=None):\n",
    "        raw_image = Image.open(path)\n",
    "        raw_image = raw_image.convert('L')\n",
    "        \n",
    "        return (np.array(raw_image, dtype=np.float32)/255.0).flatten()\n",
    "    \n",
    "    def __len__(self):\n",
    "        #return sum([len(os.listdir(os.path.join('Dataset', 'training', digit))) for digit in np.arange(0, 10).astype(str)])\n",
    "        return 54200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNet(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(FCNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_channels, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, output_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784])\n"
     ]
    }
   ],
   "source": [
    "NUM_INPUT_CHANNELS = 28 * 28\n",
    "NUM_OUTPUT_CHANNELS = 10\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-5\n",
    "MOMENTUM = 0.8\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = MNISTDataset(base_dir='.')\n",
    "\n",
    "print(train_dataset.__getitem__(1)['image'].size())\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = FCNet(input_channels=28 * 28, output_channels=10).cuda()\n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "else:\n",
    "    print('Error: Cuda was not available')\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, amsgrad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "Epoch #1\n",
      "\r",
      "\tBatch progress: 0.00% [0/1694]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amoros\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBatch progress: 5.90% [100/1694]\n",
      "\tBatch progress: 11.81% [200/1694]\n",
      "\tBatch progress: 17.71% [300/1694]\n",
      "\tBatch progress: 23.61% [400/1694]\n",
      "\tBatch progress: 29.52% [500/1694]\n",
      "\tBatch progress: 35.42% [600/1694]\n",
      "\tBatch progress: 41.32% [700/1694]\n",
      "\tBatch progress: 47.23% [800/1694]\n",
      "\tBatch progress: 53.13% [900/1694]\n",
      "\tBatch progress: 59.03% [1000/1694]\n",
      "\tBatch progress: 64.94% [1100/1694]\n",
      "\tBatch progress: 70.84% [1200/1694]\n",
      "\tBatch progress: 76.74% [1300/1694]\n",
      "\tBatch progress: 82.64% [1400/1694]\n",
      "\tBatch progress: 88.55% [1500/1694]\n",
      "\tBatch progress: 94.45% [1600/1694]\n",
      "\tLoss: 3711.62841797\tTime: 44.79150224\n",
      "Epoch #2\n",
      "\tBatch progress: 0.00% [0/1694]\n",
      "\tBatch progress: 5.90% [100/1694]\n",
      "\tBatch progress: 11.81% [200/1694]\n",
      "\tBatch progress: 17.71% [300/1694]\n",
      "\tBatch progress: 23.61% [400/1694]\n",
      "\tBatch progress: 29.52% [500/1694]\n",
      "\tBatch progress: 35.42% [600/1694]\n",
      "\tBatch progress: 41.32% [700/1694]\n",
      "\tBatch progress: 47.23% [800/1694]\n",
      "\tBatch progress: 53.13% [900/1694]\n",
      "\tBatch progress: 59.03% [1000/1694]\n",
      "\tBatch progress: 64.94% [1100/1694]\n",
      "\tBatch progress: 70.84% [1200/1694]\n",
      "\tBatch progress: 76.74% [1300/1694]\n",
      "\tBatch progress: 82.64% [1400/1694]\n",
      "\tBatch progress: 88.55% [1500/1694]\n",
      "\tBatch progress: 94.45% [1600/1694]\n",
      "\tLoss: 3156.36132812\tTime: 49.61078382\n",
      "Epoch #3\n",
      "\tBatch progress: 0.00% [0/1694]\n",
      "\tBatch progress: 5.90% [100/1694]\n",
      "\tBatch progress: 11.81% [200/1694]\n",
      "\tBatch progress: 17.71% [300/1694]\n",
      "\tBatch progress: 23.61% [400/1694]\n",
      "\tBatch progress: 29.52% [500/1694]\n",
      "\tBatch progress: 35.42% [600/1694]\n",
      "\tBatch progress: 41.32% [700/1694]\n",
      "\tBatch progress: 47.23% [800/1694]\n",
      "\tBatch progress: 53.13% [900/1694]\n",
      "\tBatch progress: 59.03% [1000/1694]\n",
      "\tBatch progress: 64.94% [1100/1694]\n",
      "\tBatch progress: 70.84% [1200/1694]\n",
      "\tBatch progress: 76.74% [1300/1694]\n",
      "\tBatch progress: 82.64% [1400/1694]\n",
      "\tBatch progress: 88.55% [1500/1694]\n",
      "\tBatch progress: 94.45% [1600/1694]\n",
      "\tLoss: 2952.19750977\tTime: 53.96617389\n",
      "Epoch #4\n",
      "\tBatch progress: 0.00% [0/1694]\n",
      "\tBatch progress: 5.90% [100/1694]\n",
      "\tBatch progress: 11.81% [200/1694]\n",
      "\tBatch progress: 17.71% [300/1694]\n",
      "\tBatch progress: 23.61% [400/1694]\n",
      "\tBatch progress: 29.52% [500/1694]\n",
      "\tBatch progress: 35.42% [600/1694]\n",
      "\tBatch progress: 41.32% [700/1694]\n",
      "\tBatch progress: 47.23% [800/1694]\n",
      "\tBatch progress: 53.13% [900/1694]\n",
      "\tBatch progress: 59.03% [1000/1694]\n",
      "\tBatch progress: 64.94% [1100/1694]\n",
      "\tBatch progress: 70.84% [1200/1694]\n",
      "\tBatch progress: 76.74% [1300/1694]\n",
      "\tBatch progress: 82.64% [1400/1694]\n",
      "\tBatch progress: 88.55% [1500/1694]\n",
      "\tBatch progress: 94.45% [1600/1694]\n",
      "\tLoss: 2835.04614258\tTime: 49.06762314\n",
      "Epoch #5\n",
      "\tBatch progress: 0.00% [0/1694]\n",
      "\tBatch progress: 5.90% [100/1694]\n",
      "\tBatch progress: 11.81% [200/1694]\n",
      "\tBatch progress: 17.71% [300/1694]\n",
      "\tBatch progress: 23.61% [400/1694]\n",
      "\tBatch progress: 29.52% [500/1694]\n",
      "\tBatch progress: 35.42% [600/1694]\n",
      "\tBatch progress: 41.32% [700/1694]\n",
      "\tBatch progress: 47.23% [800/1694]\n",
      "\tBatch progress: 53.13% [900/1694]\n",
      "\tBatch progress: 59.03% [1000/1694]\n",
      "\tBatch progress: 64.94% [1100/1694]\n",
      "\tBatch progress: 70.84% [1200/1694]\n",
      "\tBatch progress: 76.74% [1300/1694]\n",
      "\tBatch progress: 82.64% [1400/1694]\n",
      "\tBatch progress: 88.55% [1500/1694]\n",
      "\tBatch progress: 94.45% [1600/1694]\n",
      "\tLoss: 2768.87573242\tTime: 45.43726039\n",
      "Epoch #6\n",
      "\tBatch progress: 0.00% [0/1694]\n",
      "\tBatch progress: 5.90% [100/1694]\n",
      "\tBatch progress: 11.81% [200/1694]\n",
      "\tBatch progress: 17.71% [300/1694]\n",
      "\tBatch progress: 23.61% [400/1694]\n",
      "\tBatch progress: 29.52% [500/1694]\n",
      "\tBatch progress: 35.42% [600/1694]\n",
      "\tBatch progress: 41.32% [700/1694]\n",
      "\tBatch progress: 47.23% [800/1694]\n",
      "\tBatch progress: 53.13% [900/1694]\n",
      "\tBatch progress: 59.03% [1000/1694]\n",
      "\tBatch progress: 64.94% [1100/1694]\n",
      "\tBatch progress: 70.84% [1200/1694]\n",
      "\tBatch progress: 76.74% [1300/1694]\n",
      "\tBatch progress: 82.64% [1400/1694]\n",
      "\tBatch progress: 88.55% [1500/1694]\n",
      "\tBatch progress: 94.45% [1600/1694]\n",
      "\tLoss: 2732.35034180\tTime: 47.37003112\n",
      "Epoch #7\n",
      "\tBatch progress: 0.00% [0/1694]\n",
      "\tBatch progress: 5.90% [100/1694]\n",
      "\tBatch progress: 11.81% [200/1694]\n",
      "\tBatch progress: 17.71% [300/1694]\n",
      "\tBatch progress: 23.61% [400/1694]\n",
      "\tBatch progress: 29.52% [500/1694]\n",
      "\tBatch progress: 35.42% [600/1694]\n",
      "\tBatch progress: 41.32% [700/1694]\n",
      "\tBatch progress: 47.23% [800/1694]\n",
      "\tBatch progress: 53.13% [900/1694]\n",
      "\tBatch progress: 59.03% [1000/1694]\n",
      "\tBatch progress: 64.94% [1100/1694]\n",
      "\tBatch progress: 70.84% [1200/1694]\n",
      "\tBatch progress: 76.74% [1300/1694]\n",
      "\tBatch progress: 82.64% [1400/1694]\n",
      "\tBatch progress: 88.55% [1500/1694]\n",
      "\tBatch progress: 94.45% [1600/1694]\n",
      "\tLoss: 2709.37500000\tTime: 51.06125736\n",
      "Epoch #8\n",
      "\tBatch progress: 0.00% [0/1694]\n",
      "\tBatch progress: 5.90% [100/1694]\n",
      "\tBatch progress: 11.81% [200/1694]\n",
      "\tBatch progress: 17.71% [300/1694]\n",
      "\tBatch progress: 23.61% [400/1694]\n",
      "\tBatch progress: 29.52% [500/1694]\n",
      "\tBatch progress: 35.42% [600/1694]\n",
      "\tBatch progress: 41.32% [700/1694]\n",
      "\tBatch progress: 47.23% [800/1694]\n",
      "\tBatch progress: 53.13% [900/1694]\n",
      "\tBatch progress: 59.03% [1000/1694]\n",
      "\tBatch progress: 64.94% [1100/1694]\n",
      "\tBatch progress: 70.84% [1200/1694]\n",
      "\tBatch progress: 76.74% [1300/1694]\n",
      "\tBatch progress: 82.64% [1400/1694]\n",
      "\tBatch progress: 88.55% [1500/1694]\n",
      "\tBatch progress: 94.45% [1600/1694]\n",
      "\tLoss: 2693.33325195\tTime: 47.23905754\n",
      "Epoch #9\n",
      "\tBatch progress: 0.00% [0/1694]\n",
      "\tBatch progress: 5.90% [100/1694]\n",
      "\tBatch progress: 11.81% [200/1694]\n",
      "\tBatch progress: 17.71% [300/1694]\n",
      "\tBatch progress: 23.61% [400/1694]\n",
      "\tBatch progress: 29.52% [500/1694]\n",
      "\tBatch progress: 35.42% [600/1694]\n",
      "\tBatch progress: 41.32% [700/1694]\n",
      "\tBatch progress: 47.23% [800/1694]\n",
      "\tBatch progress: 53.13% [900/1694]\n",
      "\tBatch progress: 59.03% [1000/1694]\n",
      "\tBatch progress: 64.94% [1100/1694]\n",
      "\tBatch progress: 70.84% [1200/1694]\n",
      "\tBatch progress: 76.74% [1300/1694]\n",
      "\tBatch progress: 82.64% [1400/1694]\n",
      "\tBatch progress: 88.55% [1500/1694]\n",
      "\tBatch progress: 94.45% [1600/1694]\n",
      "\tLoss: 2681.34716797\tTime: 44.49460697\n",
      "Epoch #10\n",
      "\tBatch progress: 0.00% [0/1694]\n",
      "\tBatch progress: 5.90% [100/1694]\n",
      "\tBatch progress: 11.81% [200/1694]\n",
      "\tBatch progress: 17.71% [300/1694]\n",
      "\tBatch progress: 23.61% [400/1694]\n",
      "\tBatch progress: 29.52% [500/1694]\n",
      "\tBatch progress: 35.42% [600/1694]\n",
      "\tBatch progress: 41.32% [700/1694]\n",
      "\tBatch progress: 47.23% [800/1694]\n",
      "\tBatch progress: 53.13% [900/1694]\n",
      "\tBatch progress: 59.03% [1000/1694]\n",
      "\tBatch progress: 64.94% [1100/1694]\n",
      "\tBatch progress: 70.84% [1200/1694]\n",
      "\tBatch progress: 76.74% [1300/1694]\n",
      "\tBatch progress: 82.64% [1400/1694]\n",
      "\tBatch progress: 88.55% [1500/1694]\n",
      "\tBatch progress: 94.45% [1600/1694]\n",
      "\tLoss: 2671.91992188\tTime: 43.50320768\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "print(\"Training:\")\n",
    "prev_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"Epoch #{}\".format(epoch+1))\n",
    "\n",
    "    t_start = time.time()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        input_tensor, target_tensor = batch['image'], batch['label']\n",
    "\n",
    "        input_tensor = input_tensor.cuda()\n",
    "        target_tensor = target_tensor.cuda()\n",
    "\n",
    "        predicted_tensor = model(input_tensor)\n",
    "        target_tensor = torch.argmax(target_tensor, dim=1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(predicted_tensor, target_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.float()\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(\"\\r\\tBatch progress: {:.2f}% [{}/{}]\".format((idx/len(train_dataloader))*100, idx, len(train_dataloader), end=''))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    delta = time.time() - t_start\n",
    "    print(\"\\tLoss: {:.8f}\\tTime: {:.8f}\".format(running_loss, delta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), os.path.join('.', 'test.pth'))\n",
    "np.set_printoptions(precision=5)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.      0.      0.      0.      0.99979 0.      0.00018 0.      0.\n",
      " 0.00002]\n",
      "tensor(4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amoros\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "dataiter = iter(train_dataloader)\n",
    "test = dataiter.next()\n",
    "\n",
    "image_id = 0\n",
    "\n",
    "image, labels = test['image'], test['label']\n",
    "tensor = image[image_id]\n",
    "character = tensor.reshape([28,28])\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "\n",
    "fig.add_subplot(1,4,1).title.set_text('RGB image')\n",
    "plt.imshow(character.permute(0, 1))\n",
    "\n",
    "image = tensor.cuda()\n",
    "\n",
    "output = model(image)\n",
    "\n",
    "prediction = output.cpu()\n",
    "\n",
    "print(prediction.detach().numpy())\n",
    "print(prediction.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for digit in np.arange(0, 10).astype(str):\n",
    "    print(digit)\n",
    "    for idx, filename in enumerate(os.listdir(os.path.join('Dataset', 'training', digit))):\n",
    "        os.rename(os.path.join('Dataset', 'training', digit, filename), os.path.join('Dataset', 'training', digit, str(idx) + '.png'))\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54200\n"
     ]
    }
   ],
   "source": [
    "print (sum([len(os.listdir(os.path.join('Dataset', 'training', digit))) for digit in np.arange(0, 10).astype(str)]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
